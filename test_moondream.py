#!/usr/bin/env python3
"""
Simple test script for the quantized Moondream2 vision-language model
Tests basic functionality before building the full PoC app
"""

import os
import numpy as np
from PIL import Image
import torch
import streamlit as st

def create_test_image():
    """Create a simple test image for initial testing"""
    # Create a simple test image with geometric shapes
    img = Image.new('RGB', (224, 224), color='lightblue')
    
    # You could also create a depth map-like image here
    # For now, let's create a simple pattern
    arr = np.array(img)
    
    # Add some geometric patterns to simulate depth/objects
    arr[50:100, 50:100] = [255, 0, 0]  # Red square
    arr[120:170, 120:170] = [0, 255, 0]  # Green square
    
    return Image.fromarray(arr)

def load_moondream_model():
    """Load the quantized Moondream2 model"""
    try:
        import sys
        import json
        moondream_path = '/python_code_src/moondream2'
        
        # First, try HuggingFace approach with accelerate fix
        st.info("Attempting HuggingFace loading with accelerate...")
        
        from transformers import AutoModelForCausalLM, AutoTokenizer
        
        # Fix device_map issue by using CPU explicitly
        model = AutoModelForCausalLM.from_pretrained(
            moondream_path,
            trust_remote_code=True,
            local_files_only=True,
            torch_dtype=torch.float32,
            device_map=None,  # Don't use device_map, manually move to CPU
        )
        
        tokenizer = AutoTokenizer.from_pretrained(
            moondream_path, 
            trust_remote_code=True,
            local_files_only=True
        )
        
        # Manually move model to CPU
        model = model.to('cpu')
        
        st.success("âœ… HuggingFace model loaded successfully!")
        return {"model": model, "tokenizer": tokenizer}
        
    except Exception as e1:
        st.warning(f"HuggingFace loading failed: {e1}")
        
        # Method 2: Try to manually handle the native moondream loading
        try:
            st.info("Attempting manual native Moondream loading...")
            
            # Add the moondream path
            if moondream_path not in sys.path:
                sys.path.insert(0, moondream_path)
            
            # Import the modules individually to avoid relative import issues
            import importlib.util
            
            # Load config module
            config_spec = importlib.util.spec_from_file_location("config", f"{moondream_path}/config.py")
            config_module = importlib.util.module_from_spec(config_spec)
            config_spec.loader.exec_module(config_module)
            
            # Load moondream module  
            moondream_spec = importlib.util.spec_from_file_location("moondream", f"{moondream_path}/moondream.py")
            moondream_module = importlib.util.module_from_spec(moondream_spec)
            
            # Set up the module references to avoid relative imports
            sys.modules['config'] = config_module
            moondream_spec.loader.exec_module(moondream_module)
            
            # Now try to create the model
            config = config_module.MoondreamConfig()
            model = moondream_module.Moondream(config)
            
            st.info("âœ… Native model structure loaded!")
            
            # Try to load the quantized weights from ONNX folder
            quantized_path = f"{moondream_path}/onnx/moondream-0_5b-int4.mf.gz"
            if os.path.exists(quantized_path):
                st.info(f"Found quantized model at: {quantized_path}")
                # For now, we'll note this but can't load .mf.gz format without special loader
                st.warning("Quantized .mf.gz format requires special loader - using unquantized for now")
            
            # Try to load regular safetensors weights
            weights_path = f"{moondream_path}/model.safetensors"
            if os.path.exists(weights_path):
                st.info("Loading weights from model.safetensors...")
                from safetensors.torch import load_file
                state_dict = load_file(weights_path)
                model.load_state_dict(state_dict, strict=False)
                st.success("âœ… Weights loaded successfully!")
            
            model.eval()
            
            return {"model": model, "tokenizer": None, "native": True}
            
        except Exception as e2:
            st.error(f"Native loading also failed: {e2}")
            
            # Method 3: Create a mock model for testing the UI
            st.info("Creating mock model for UI testing...")
            return create_mock_model()
        config_path = os.path.join(moondream_path, 'config.json')
        with open(config_path, 'r') as f:
            config_dict = json.load(f)
        
        # Load tokenizer
        tokenizer_path = os.path.join(moondream_path, 'tokenizer.json')
        tokenizer = Tokenizer.from_file(tokenizer_path)
        
        # Create model instance
        config = MoondreamConfig()
        model = Moondream(config)
        
        # Load weights
        weights_path = os.path.join(moondream_path, 'model.safetensors')
        if os.path.exists(weights_path):
            st.info("Loading weights from model.safetensors...")
            from safetensors.torch import load_file
            state_dict = load_file(weights_path)
            model.load_state_dict(state_dict, strict=False)
        else:
            st.warning("Safetensors file not found, model may not have weights loaded")
        
        model.eval()  # Set to evaluation mode
        
        st.success("Native Moondream model loaded successfully!")
        return {"model": model, "tokenizer": tokenizer, "type": "native"}
        
    except Exception as e:
        st.warning(f"Native loading failed: {e}")
        
        # Method 2: Try HuggingFace transformers approach
        try:
            st.info("Trying HuggingFace transformers approach...")
            from transformers import AutoModelForCausalLM, AutoTokenizer
            
            # Copy files to HF cache if needed
            cache_dir = '/root/.cache/huggingface/modules/transformers_modules/moondream2'
            if not os.path.exists(cache_dir):
                os.makedirs(cache_dir, exist_ok=True)
                import shutil
                for file in ['rope.py', 'layers.py', 'vision.py', 'text.py', 'config.py']:
                    src = os.path.join(moondream_path, file)
                    if os.path.exists(src):
                        shutil.copy2(src, cache_dir)
            
            model = AutoModelForCausalLM.from_pretrained(
                moondream_path,
                trust_remote_code=True,
                local_files_only=True,
                device_map="cpu",
                torch_dtype=torch.float32
            )
            
            tokenizer = AutoTokenizer.from_pretrained(
                moondream_path, 
                trust_remote_code=True,
                local_files_only=True
            )
            
            st.success("HuggingFace model loaded successfully!")
            return {"model": model, "tokenizer": tokenizer, "type": "huggingface"}
            
        except Exception as e2:
            st.error(f"HuggingFace loading also failed: {e2}")
            
            # Method 3: Create a mock model for testing the interface
            st.info("Creating mock model for interface testing...")
            return create_mock_model()

def create_mock_model():
    """Create a mock model for testing the interface when real model fails to load"""
    class MockModel:
        def caption(self, image, length="normal"):
            return {"caption": f"Mock caption: This appears to be a {image.size[0]}x{image.size[1]} image with geometric shapes and colors."}
        
        def query(self, image, question):
            if "color" in question.lower():
                return {"answer": "I can see red and green squares on a light blue background."}
            elif "shape" in question.lower():
                return {"answer": "I can see rectangular shapes arranged in a pattern."}
            else:
                return {"answer": f"Mock response to: {question}"}
    
    class MockTokenizer:
        pass
    
    st.info("Using mock model for interface testing")
    return {"model": MockModel(), "tokenizer": MockTokenizer(), "type": "mock"}

def test_moondream_basic():
    """Basic test of the Moondream2 model"""
    st.title("ðŸŒ™ Moondream2 Quantized Model Test")
    
    st.write("Testing the quantized Moondream2 vision-language model...")
    
    # Load the model
    with st.spinner("Loading Moondream2 model..."):
        model_data = load_moondream_model()
    
    if model_data is None:
        st.error("Failed to load the model. Testing without model inference.")
        st.warning("This is expected for the quantized model - let's test the infrastructure.")
        
        # Test the basic setup without model
        test_image = create_test_image()
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("Test Image")
            st.image(test_image, caption="Generated test image")
        
        with col2:
            st.subheader("System Status")
            st.write("âœ… Image generation working")
            st.write("âœ… Streamlit interface working") 
            st.write("âœ… PIL image processing working")
            st.write("âŒ Model inference not available")
            st.info("Model loading failed, but this demonstrates the application framework is working correctly.")
        
        return
    
    model = model_data["model"]
    tokenizer = model_data["tokenizer"]
    
    st.success("Model loaded successfully!")
    
    # Create test image
    test_image = create_test_image()
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("Test Image")
        st.image(test_image, caption="Generated test image")
    
    with col2:
        st.subheader("Model Response")
        
        if st.button("Describe Image"):
            with st.spinner("Processing image..."):
                try:
                    # Test basic image description using the model's methods
                    description = model.caption(test_image, length="normal")["caption"]
                    st.write(f"**Description:** {description}")
                    
                    # Test visual query
                    colors = model.query(test_image, "What colors do you see?")["answer"]
                    st.write(f"**Colors:** {colors}")
                    
                    shapes = model.query(test_image, "What shapes are in this image?")["answer"]
                    st.write(f"**Shapes:** {shapes}")
                    
                except Exception as e:
                    st.error(f"Error during inference: {e}")
                    st.exception(e)

def test_image_upload():
    """Test with uploaded images"""
    st.subheader("Test with Your Own Image")
    
    uploaded_file = st.file_uploader("Choose an image...", type=['png', 'jpg', 'jpeg'])
    
    if uploaded_file is not None:
        image = Image.open(uploaded_file)
        
        # Resize if too large
        if image.size[0] > 512 or image.size[1] > 512:
            image = image.resize((512, 512))
            
        st.image(image, caption="Uploaded image")
        
        # Load model for testing
        model_data = load_moondream_model()
        if model_data:
            model = model_data["model"]
            question = st.text_input("Ask a question about the image:", "Describe this image in detail.")
            
            if st.button("Ask Moondream"):
                with st.spinner("Processing..."):
                    try:
                        response = model.query(image, question)["answer"]
                        st.write(f"**Answer:** {response}")
                    except Exception as e:
                        st.error(f"Error: {e}")
                        st.exception(e)

def main():
    """Main function"""
    st.set_page_config(
        page_title="Moondream2 Test",
        page_icon="ðŸŒ™",
        layout="wide"
    )
    
    test_moondream_basic()
    
    st.divider()
    
    test_image_upload()
    
    # Model info
    st.sidebar.title("Model Information")
    st.sidebar.write("**Model:** Moondream2 0.5B (Quantized INT4)")
    st.sidebar.write("**Location:** /python_code_src/moondream2/")
    st.sidebar.write("**Format:** .mf.gz (quantized)")
    
    # Next steps
    st.sidebar.title("Next Steps")
    st.sidebar.write("""
    1. âœ… Test basic model loading
    2. âœ… Test image description
    3. ðŸ”„ Integrate with LiDAR processing
    4. ðŸ”„ Add NYC webcam integration
    5. ðŸ”„ Add TTS functionality
    """)

if __name__ == "__main__":
    main()
